<!DOCTYPE html>
<html lang="en">
  <head>
    <title>TITLE</title>
  
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />

        <!-- 
      Create box parallax
      1: AFrame & Object model display => aframe.html
      2: Webcam & Tensorflow test <--here
      3: Create logic for parallax
     -->

     
  </head>
  
  <body>
    @@include('./inc/header.html')

    <style>
      .container {
        position: fixed;
        top: 0;
        right: 0;
        bottom: 0;
        left: 0;
        width: 100%;
        height: 100%;
        z-index: 5;
        object-fit: cover;
      }
    </style>



    <main>
      <video class="video" autoplay></video>
    </main>


        <!-- Tensorflow -->
    <!-- <script src="https://unpkg.com/@tensorflow/tfjs"></script> -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
<script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@2.1.0/dist/tf-backend-webgl.js"></script>  -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>

    <!-- <script src="https://unpkg.com/@tensorflow/tfjs-core@2.1.0/dist/tf-core.js"></script>
    <script src="https://unpkg.com/@tensorflow/tfjs-converter@2.1.0/dist/tf-converter.js"></script>
    -->
    <!-- <script src="./utils/utilities.js"></script> -->

    <script>
      // Webcam setting
      const video = document.querySelector('.video')
      function startup() {
          navigator.mediaDevices.getUserMedia({
            audio: false,
            video: true
          }).then(stream => {
            video.srcObject = stream

          }).catch(console.log(console.error))
      }
      startup()
    </script>

    <script>
      async function main() {
        // Load the MediaPipe facemesh model.
        const model = facemesh.load()

        // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain an
        // array of detected faces from the MediaPipe graph.
        // const predictions = await model.estimateFaces(video)

        // if (predictions.length > 0) {
        //   for (let i = 0; i < predictions.length; i++) {
        //     const keypoints = predictions[i].scaledMesh

        //     // Log facial keypoints.
        //     for (let i = 0; i < keypoints.length; i++) {
        //       const [x, y, z] = keypoints[i]

        //       console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`)
        //     }
        //   }
        // }
      }

      main()
    </script>
  </body>
</html>
